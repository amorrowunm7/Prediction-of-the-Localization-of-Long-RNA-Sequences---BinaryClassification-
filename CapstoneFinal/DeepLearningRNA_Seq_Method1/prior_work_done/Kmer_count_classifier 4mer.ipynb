{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./NuclearCytosolLncRNAs_ALL_4mer_stride1_tokens.csv.count.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>tttt</th>\n",
       "      <th>agaa</th>\n",
       "      <th>cctg</th>\n",
       "      <th>ccag</th>\n",
       "      <th>ctgg</th>\n",
       "      <th>aaat</th>\n",
       "      <th>gaaa</th>\n",
       "      <th>cagg</th>\n",
       "      <th>...</th>\n",
       "      <th>acga</th>\n",
       "      <th>ttcg</th>\n",
       "      <th>tcgt</th>\n",
       "      <th>cgac</th>\n",
       "      <th>gtcg</th>\n",
       "      <th>tcga</th>\n",
       "      <th>atcg</th>\n",
       "      <th>cgat</th>\n",
       "      <th>tacg</th>\n",
       "      <th>cgta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000589451.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000490351.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000527727.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000612724.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000634547.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     class  aaaa  tttt  agaa  cctg  ccag  ctgg  aaat  gaaa  \\\n",
       "name                                                                         \n",
       "ENST00000589451.1  Nuclear     0     0     3     8    10    12     1     2   \n",
       "ENST00000490351.1  Nuclear     8     5     9     1     4     2     4     5   \n",
       "ENST00000527727.1  Nuclear     8     4     4     3     4     6     4     6   \n",
       "ENST00000612724.1  Nuclear     6     4     4     2     4     4     2     2   \n",
       "ENST00000634547.1  Nuclear     5     6     3    12    12    14     5     7   \n",
       "\n",
       "                   cagg  ...   acga  ttcg  tcgt  cgac  gtcg  tcga  atcg  cgat  \\\n",
       "name                     ...                                                    \n",
       "ENST00000589451.1     7  ...      0     0     0     0     0     0     0     0   \n",
       "ENST00000490351.1     2  ...      2     0     0     1     0     1     0     0   \n",
       "ENST00000527727.1     3  ...      0     0     0     0     0     0     0     0   \n",
       "ENST00000612724.1     1  ...      0     0     0     0     2     0     0     0   \n",
       "ENST00000634547.1    10  ...      1     1     1     0     2     0     0     2   \n",
       "\n",
       "                   tacg  cgta  \n",
       "name                           \n",
       "ENST00000589451.1     0     0  \n",
       "ENST00000490351.1     0     0  \n",
       "ENST00000527727.1     0     0  \n",
       "ENST00000612724.1     0     0  \n",
       "ENST00000634547.1     1     1  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8662, 257)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the classes to 0 and 1\n",
    "df['class'] = pd.factorize(df['class'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strtify sample to train , test and validation data\n",
    "# Validation data is important for neural network backpropagation\n",
    "X=df.drop(columns=['class'])\n",
    "y=df.loc[:,'class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=15)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=0.15, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SparsePCA is this running a Principal Components Analysis for feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import SparsePCA,PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pipeline is a list of preprocessing steps in one command\n",
    "# first we will remove high correlated features\n",
    "# then we will apply scaling as neural network needs values frm 0 to 1\n",
    "pipeline = Pipeline([('corr-removal',VarianceThreshold(0.8)),\n",
    "                     #('selector',SelectKBest(chi2,100)),\n",
    "                     ('scaler',MinMaxScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amorrow\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Applying the pipeline\n",
    "x_train = pipeline.fit_transform(X_train,y_train)\n",
    "# notice that the pipeline learn the best parameters from the tarin data only to apply to test ana valiadation data\n",
    "x_test = pipeline.transform(X_test)\n",
    "x_valid = pipeline.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/05/cd/c171d2e33c0192b04560ce864c26eba83fed888fe5cd9ded661b2702f2ae/tensorflow-1.12.0-cp36-cp36m-win_amd64.whl (45.9MB)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\amorrow\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.7)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/a7/fa027f411616f6e7c5899a8e3e1ab2e101808f8d62b6ee8b645411ed270b/grpcio-1.18.0-cp36-cp36m-win_amd64.whl (1.5MB)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/df/d606d07cff0fc8d22abcc54006c0247002d11a7f2d218eb008d48e76851d/protobuf-3.6.1-cp36-cp36m-win_amd64.whl (1.1MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\amorrow\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\amorrow\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.1)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/31/bc/ab68120d1d89ae23b694a55fe2aece2f91194313b71f9b05a80b32d3c24b/absl-py-0.7.0.tar.gz (96kB)\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\amorrow\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.9)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\amorrow\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\amorrow\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amorrow\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\amorrow\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
      "Building wheels for collected packages: termcolor, gast, absl-py\n",
      "  Running setup.py bdist_wheel for termcolor: started\n",
      "  Running setup.py bdist_wheel for termcolor: finished with status 'error'\n",
      "  Complete output from command C:\\Users\\amorrow\\AppData\\Local\\Continuum\\anaconda3\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\amorrow\\\\AppData\\\\Local\\\\Temp\\\\pip-install-jbzobkl8\\\\termcolor\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d C:\\Users\\amorrow\\AppData\\Local\\Temp\\pip-wheel-g76wuclr --python-tag cp36:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib\n",
      "  copying termcolor.py -> build\\lib\n",
      "  installing to build\\bdist.win-amd64\\wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build\\bdist.win-amd64\n",
      "  creating build\\bdist.win-amd64\\wheel\n",
      "  copying build\\lib\\termcolor.py -> build\\bdist.win-amd64\\wheel\\.\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  creating termcolor.egg-info\n",
      "  writing termcolor.egg-info\\PKG-INFO\n",
      "  writing dependency_links to termcolor.egg-info\\dependency_links.txt\n",
      "  writing top-level names to termcolor.egg-info\\top_level.txt\n",
      "  writing manifest file 'termcolor.egg-info\\SOURCES.txt'\n",
      "  reading manifest file 'termcolor.egg-info\\SOURCES.txt'\n",
      "  writing manifest file 'termcolor.egg-info\\SOURCES.txt'\n",
      "  Copying termcolor.egg-info to build\\bdist.win-amd64\\wheel\\.\\termcolor-1.1.0-py3.6.egg-info\n",
      "  running install_scripts\n",
      "  creating build\\bdist.win-amd64\\wheel\\termcolor-1.1.0.dist-info\\WHEEL\n",
      "  creating 'C:\\Users\\amorrow\\AppData\\Local\\Temp\\pip-wheel-g76wuclr\\termcolor-1.1.0-cp36-none-any.whl' and adding '.' to it\n",
      "  adding 'termcolor.py'\n",
      "  adding 'termcolor-1.1.0.dist-info\\top_level.txt'\n",
      "  adding 'termcolor-1.1.0.dist-info\\WHEEL'\n",
      "  adding 'termcolor-1.1.0.dist-info\\METADATA'\n",
      "  adding 'termcolor-1.1.0.dist-info\\RECORD'\n",
      "  removing build\\bdist.win-amd64\\wheel\n",
      "  error: [WinError 145] The directory is not empty: 'build\\\\bdist.win-amd64\\\\wheel\\\\termcolor-1.1.0.dist-info'\n",
      "  \n",
      "  ----------------------------------------\n",
      "  Running setup.py clean for termcolor\n",
      "  Running setup.py bdist_wheel for gast: started\n",
      "  Running setup.py bdist_wheel for gast: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\amorrow\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Running setup.py bdist_wheel for absl-py: started\n",
      "  Running setup.py bdist_wheel for absl-py: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\amorrow\\AppData\\Local\\pip\\Cache\\wheels\\90\\db\\f8\\2c3101f72ef1ad434e4662853174126ce30201a3e163dcbeca\n",
      "Successfully built gast absl-py\n",
      "Failed to build termcolor\n",
      "Installing collected packages: termcolor, grpcio, protobuf, gast, absl-py, markdown, tensorboard, astor, tensorflow\n",
      "  Running setup.py install for termcolor: started\n",
      "    Running setup.py install for termcolor: finished with status 'done'\n",
      "Successfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.18.0 markdown-3.0.1 protobuf-3.6.1 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Failed building wheel for termcolor\n",
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-8251ee19c170>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-28-8251ee19c170>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m pip install --upgrade pip' command.\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m pip install --upgrade pip' command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, Conv1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, LeakyReLU, concatenate, MaxPool1D,GlobalMaxPool1D,add\n",
    "from keras.layers import Dense, Embedding, Input, Masking, Dropout, MaxPooling1D,Lambda, BatchNormalization\n",
    "from keras.layers import LSTM, TimeDistributed, AveragePooling1D, Flatten,Activation,ZeroPadding1D, UpSampling1D\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping,ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, ConvLSTM2D, Bidirectional,RepeatVector\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid the MLP neural network \n",
    "def build_mlp_classifier():\n",
    "    #the input must have the number of features\n",
    "    inp1 = Input(shape=(x_train.shape[1],),dtype='float')\n",
    "    main = Dense(64)(inp1)\n",
    "    # Dropouts are important t prevent the overfitting\n",
    "    main = Dropout(0.5)(main)\n",
    "    # Batch normalization allow faster convergence\n",
    "    main = BatchNormalization()(main)\n",
    "    main = Dense(64)(main)  \n",
    "    main = Dropout(0.5)(main)\n",
    "    main = BatchNormalization()(main)\n",
    "    main = Dense(32)(main)  \n",
    "    #main = Dropout(0.5)(main)\n",
    "    #main = Dense(256)(main)  \n",
    "    #main = Dropout(0.5)(main)\n",
    "    # Simoid function is a must in binary classification\n",
    "    out = Dense(2,activation='sigmoid')(main)\n",
    "    model = Model(inputs=[inp1], outputs=[out])\n",
    "    # Slower leraning rate is important in small dataset < 0.001\n",
    "    optimizer = Adam(lr=0.0001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the output to one hot encoding\n",
    "y_train_ = to_categorical(y_train,num_classes=2)\n",
    "y_test_ = to_categorical(y_test,num_classes=2)\n",
    "y_valid_ = to_categorical(y_valid,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/1000\n",
      " - 2s - loss: 0.8457 - acc: 0.5184 - val_loss: 0.7023 - val_acc: 0.5229\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 0.8114 - acc: 0.5453 - val_loss: 0.6926 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 0.7814 - acc: 0.5493 - val_loss: 0.6829 - val_acc: 0.5563\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.7606 - acc: 0.5475 - val_loss: 0.6782 - val_acc: 0.5750\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.7297 - acc: 0.5762 - val_loss: 0.6776 - val_acc: 0.5729\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.7271 - acc: 0.5633 - val_loss: 0.6705 - val_acc: 0.5771\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.7357 - acc: 0.5740 - val_loss: 0.6681 - val_acc: 0.5771\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 0.7308 - acc: 0.5820 - val_loss: 0.6653 - val_acc: 0.5750\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 0.7345 - acc: 0.5636 - val_loss: 0.6605 - val_acc: 0.5771\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 0.6980 - acc: 0.6026 - val_loss: 0.6595 - val_acc: 0.5875\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 0.7057 - acc: 0.5872 - val_loss: 0.6582 - val_acc: 0.5958\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 0.6946 - acc: 0.5993 - val_loss: 0.6520 - val_acc: 0.5917\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 0.6917 - acc: 0.5920 - val_loss: 0.6520 - val_acc: 0.5938\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 0.6908 - acc: 0.5890 - val_loss: 0.6521 - val_acc: 0.6062\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 0.6766 - acc: 0.6174 - val_loss: 0.6507 - val_acc: 0.6104\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 0.6721 - acc: 0.6148 - val_loss: 0.6478 - val_acc: 0.6125\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 0.6638 - acc: 0.6148 - val_loss: 0.6493 - val_acc: 0.6208\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 0.6624 - acc: 0.6141 - val_loss: 0.6490 - val_acc: 0.6188\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 0.6582 - acc: 0.6152 - val_loss: 0.6449 - val_acc: 0.6125\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 0.6578 - acc: 0.6148 - val_loss: 0.6433 - val_acc: 0.6188\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 0.6657 - acc: 0.6082 - val_loss: 0.6422 - val_acc: 0.6208\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 0.6619 - acc: 0.6210 - val_loss: 0.6419 - val_acc: 0.6167\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 0.6527 - acc: 0.6310 - val_loss: 0.6408 - val_acc: 0.6292\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 0.6464 - acc: 0.6192 - val_loss: 0.6401 - val_acc: 0.6333\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 0.6562 - acc: 0.6251 - val_loss: 0.6398 - val_acc: 0.6333\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 0.6433 - acc: 0.6313 - val_loss: 0.6379 - val_acc: 0.6312\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 0.6400 - acc: 0.6266 - val_loss: 0.6374 - val_acc: 0.6354\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 0.6412 - acc: 0.6258 - val_loss: 0.6361 - val_acc: 0.6333\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 0.6519 - acc: 0.6170 - val_loss: 0.6361 - val_acc: 0.6333\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 0.6261 - acc: 0.6501 - val_loss: 0.6362 - val_acc: 0.6188\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 0.6405 - acc: 0.6358 - val_loss: 0.6361 - val_acc: 0.6146\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 0.6308 - acc: 0.6358 - val_loss: 0.6363 - val_acc: 0.6229\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 0.6335 - acc: 0.6350 - val_loss: 0.6380 - val_acc: 0.6167\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 0.6386 - acc: 0.6280 - val_loss: 0.6363 - val_acc: 0.6292\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 0.6204 - acc: 0.6431 - val_loss: 0.6342 - val_acc: 0.6312\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 0.6286 - acc: 0.6347 - val_loss: 0.6345 - val_acc: 0.6375\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.6301 - acc: 0.6387 - val_loss: 0.6367 - val_acc: 0.6292\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.6128 - acc: 0.6512 - val_loss: 0.6354 - val_acc: 0.6375\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.6256 - acc: 0.6497 - val_loss: 0.6352 - val_acc: 0.6333\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.6243 - acc: 0.6420 - val_loss: 0.6346 - val_acc: 0.6292\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.6081 - acc: 0.6593 - val_loss: 0.6342 - val_acc: 0.6417\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.6229 - acc: 0.6564 - val_loss: 0.6356 - val_acc: 0.6375\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.6143 - acc: 0.6589 - val_loss: 0.6338 - val_acc: 0.6292\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.6262 - acc: 0.6428 - val_loss: 0.6339 - val_acc: 0.6354\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.6190 - acc: 0.6542 - val_loss: 0.6371 - val_acc: 0.6312\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.6142 - acc: 0.6582 - val_loss: 0.6365 - val_acc: 0.6292\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.6168 - acc: 0.6457 - val_loss: 0.6365 - val_acc: 0.6292\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.6129 - acc: 0.6582 - val_loss: 0.6351 - val_acc: 0.6312\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.6108 - acc: 0.6516 - val_loss: 0.6329 - val_acc: 0.6250\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.6015 - acc: 0.6714 - val_loss: 0.6344 - val_acc: 0.6354\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.6155 - acc: 0.6762 - val_loss: 0.6348 - val_acc: 0.6333\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.6212 - acc: 0.6582 - val_loss: 0.6324 - val_acc: 0.6312\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.6112 - acc: 0.6556 - val_loss: 0.6350 - val_acc: 0.6333\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.6130 - acc: 0.6681 - val_loss: 0.6348 - val_acc: 0.6396\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.6144 - acc: 0.6461 - val_loss: 0.6338 - val_acc: 0.6375\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.6119 - acc: 0.6678 - val_loss: 0.6361 - val_acc: 0.6396\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.6017 - acc: 0.6707 - val_loss: 0.6336 - val_acc: 0.6312\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.5989 - acc: 0.6843 - val_loss: 0.6329 - val_acc: 0.6312\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.6195 - acc: 0.6722 - val_loss: 0.6325 - val_acc: 0.6375\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.6106 - acc: 0.6575 - val_loss: 0.6327 - val_acc: 0.6500\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.6037 - acc: 0.6792 - val_loss: 0.6342 - val_acc: 0.6458\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.6031 - acc: 0.6711 - val_loss: 0.6326 - val_acc: 0.6479\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.6046 - acc: 0.6711 - val_loss: 0.6325 - val_acc: 0.6479\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.6003 - acc: 0.6707 - val_loss: 0.6326 - val_acc: 0.6438\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.6065 - acc: 0.6751 - val_loss: 0.6320 - val_acc: 0.6458\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 0.6088 - acc: 0.6604 - val_loss: 0.6318 - val_acc: 0.6354\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 0.6087 - acc: 0.6623 - val_loss: 0.6320 - val_acc: 0.6438\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 0.6012 - acc: 0.6803 - val_loss: 0.6329 - val_acc: 0.6438\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 0.5993 - acc: 0.6696 - val_loss: 0.6321 - val_acc: 0.6375\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 0.6090 - acc: 0.6667 - val_loss: 0.6317 - val_acc: 0.6375\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 0.6115 - acc: 0.6637 - val_loss: 0.6320 - val_acc: 0.6354\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 0.6011 - acc: 0.6818 - val_loss: 0.6317 - val_acc: 0.6333\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 0.5972 - acc: 0.6818 - val_loss: 0.6317 - val_acc: 0.6354\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 0.6088 - acc: 0.6733 - val_loss: 0.6312 - val_acc: 0.6375\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 0.6046 - acc: 0.6770 - val_loss: 0.6318 - val_acc: 0.6375\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 0.6067 - acc: 0.6744 - val_loss: 0.6325 - val_acc: 0.6542\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 0.6154 - acc: 0.6726 - val_loss: 0.6315 - val_acc: 0.6438\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 0.6114 - acc: 0.6431 - val_loss: 0.6323 - val_acc: 0.6562\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 0.5940 - acc: 0.6818 - val_loss: 0.6320 - val_acc: 0.6521\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 0.5966 - acc: 0.6880 - val_loss: 0.6321 - val_acc: 0.6542\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 0.6103 - acc: 0.6829 - val_loss: 0.6319 - val_acc: 0.6562\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 0.6018 - acc: 0.6740 - val_loss: 0.6318 - val_acc: 0.6542\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 0.5929 - acc: 0.6781 - val_loss: 0.6316 - val_acc: 0.6542\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 0.5960 - acc: 0.6795 - val_loss: 0.6318 - val_acc: 0.6542\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 0.6033 - acc: 0.6718 - val_loss: 0.6319 - val_acc: 0.6542\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 0.6036 - acc: 0.6762 - val_loss: 0.6324 - val_acc: 0.6542\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 0.5993 - acc: 0.6843 - val_loss: 0.6326 - val_acc: 0.6583\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 0.6132 - acc: 0.6722 - val_loss: 0.6320 - val_acc: 0.6542\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 0.6017 - acc: 0.6634 - val_loss: 0.6320 - val_acc: 0.6521\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 0.5950 - acc: 0.6810 - val_loss: 0.6322 - val_acc: 0.6562\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 0.5902 - acc: 0.6810 - val_loss: 0.6321 - val_acc: 0.6542\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 0.6099 - acc: 0.6689 - val_loss: 0.6320 - val_acc: 0.6521\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 0.6044 - acc: 0.6737 - val_loss: 0.6318 - val_acc: 0.6479\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 0.5948 - acc: 0.6795 - val_loss: 0.6317 - val_acc: 0.6542\n",
      "Epoch 95/1000\n",
      " - 0s - loss: 0.5876 - acc: 0.6773 - val_loss: 0.6321 - val_acc: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1000\n",
      " - 0s - loss: 0.5999 - acc: 0.6685 - val_loss: 0.6321 - val_acc: 0.6521\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 0.5956 - acc: 0.6784 - val_loss: 0.6321 - val_acc: 0.6583\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 0.5992 - acc: 0.6979 - val_loss: 0.6322 - val_acc: 0.6521\n",
      "Epoch 99/1000\n",
      " - 0s - loss: 0.5998 - acc: 0.6770 - val_loss: 0.6322 - val_acc: 0.6583\n",
      "Epoch 100/1000\n",
      " - 0s - loss: 0.5973 - acc: 0.6854 - val_loss: 0.6324 - val_acc: 0.6458\n",
      "Epoch 101/1000\n",
      " - 0s - loss: 0.5944 - acc: 0.6840 - val_loss: 0.6321 - val_acc: 0.6583\n",
      "Epoch 102/1000\n",
      " - 0s - loss: 0.5988 - acc: 0.6818 - val_loss: 0.6323 - val_acc: 0.6500\n",
      "Epoch 103/1000\n",
      " - 0s - loss: 0.5982 - acc: 0.6707 - val_loss: 0.6320 - val_acc: 0.6500\n",
      "Epoch 104/1000\n",
      " - 0s - loss: 0.5995 - acc: 0.6685 - val_loss: 0.6318 - val_acc: 0.6479\n",
      "400/400 [==============================] - 0s 53us/step\n",
      "MLP Model Accuracy : 0.62\n"
     ]
    }
   ],
   "source": [
    "# MLP model\n",
    "batch_size = 32\n",
    "model = build_mlp_classifier()\n",
    "# check point to save the model at best epoch\n",
    "checkpoint = ModelCheckpoint(filepath='MLP.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "#early top to stop the model if don't converge in 30 epochs\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=30,\n",
    "                          verbose=0, mode='auto')\n",
    "# RL to decrease the learning rate if the model didn't converge in 10 epochs\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=10)\n",
    "model.fit(x_train,y_train_,callbacks=[checkpoint,RL,EarlyStop],\n",
    "          validation_data=[x_valid, y_valid_],\n",
    "          epochs=1000,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "# loading the bestw weights\n",
    "model.load_weights('MLP.hdfs')\n",
    "\n",
    "# Calcualting the accuracy\n",
    "acc = model.evaluate(x_test,y_test_)[1]\n",
    "print('MLP Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing some simple ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6410848240046163"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_test)\n",
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6739757645701097"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_test)\n",
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tpot apply genetic algorithm for hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes alot of time\n",
    "# you will need to install tpot first pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biohacker2/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77ec2bcf33143d18c33846e5ee265ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=410), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new folder to save periodic pipeline: tpot_results.txt\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-08-24.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.6474036703083503\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=7, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "-2\t0.6556923174784919\tKNeighborsClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=1, XGBClassifier__min_child_weight=9, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001), KNeighborsClassifier__n_neighbors=62, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-09-18.py\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #29 due to time out. Continuing to the next pipeline.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.6504476170030056\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "-2\t0.6556923174784919\tKNeighborsClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=1, XGBClassifier__min_child_weight=9, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001), KNeighborsClassifier__n_neighbors=62, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.6504476170030056\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "-2\t0.6614667716234913\tKNeighborsClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=1, XGBClassifier__min_child_weight=13, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001), KNeighborsClassifier__n_neighbors=62, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-16-38.py\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.6512208784568168\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "-2\t0.6637376158898272\tKNeighborsClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=1, XGBClassifier__min_child_weight=10, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001), KNeighborsClassifier__n_neighbors=62, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-18-05.py\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.6526265842914619\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6637376158898272\tKNeighborsClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.5, XGBClassifier__max_depth=1, XGBClassifier__min_child_weight=10, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001), KNeighborsClassifier__n_neighbors=62, KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.6644405483198957\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-25-44.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.6644405483198957\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.6644405483198957\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.6658735667827552\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.15000000000000002)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-36-46.py\n",
      "Skipped pipeline #112 due to time out. Continuing to the next pipeline.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.6659114546061733\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=10, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.5)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-42-58.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 11 - Current Pareto front scores:\n",
      "-1\t0.6659114546061733\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=10, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.5)\n",
      "-2\t0.6673550482642367\tXGBClassifier(ZeroCount(input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.15000000000000002)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-44-55.py\n",
      "Generation 12 - Current Pareto front scores:\n",
      "-1\t0.6673549687514908\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.2)\n",
      "-2\t0.6673550482642367\tXGBClassifier(ZeroCount(input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.15000000000000002)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The condensed distance matrix must contain only finite values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped pipeline #138 due to time out. Continuing to the next pipeline.\n",
      "Generation 13 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-52-18.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 14 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 15 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6739838271074853\tXGBClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=15, ExtraTreesClassifier__n_estimators=100), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.25)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_20-58-47.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Generation 16 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6754707552120605\tRandomForestClassifier(ZeroCount(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_21-01-18.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "Skipped pipeline #178 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #182 due to time out. Continuing to the next pipeline.\n",
      "Generation 17 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6754707552120605\tRandomForestClassifier(ZeroCount(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Generation 18 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6768871952674014\tXGBClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_21-10-15.py\n",
      "Generation 19 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6768871952674014\tXGBClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-4\t0.6769684175373312\tXGBClassifier(CombineDFs(RFE(StandardScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.3, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.001)), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_21-12-43.py\n",
      "Generation 20 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6768871952674014\tXGBClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-4\t0.6769684175373312\tXGBClassifier(CombineDFs(RFE(StandardScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.3, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.001)), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Generation 21 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6768871952674014\tXGBClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-4\t0.6769684175373312\tXGBClassifier(CombineDFs(RFE(StandardScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.3, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.001)), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 22 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6768871952674014\tXGBClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-4\t0.6769684175373312\tXGBClassifier(CombineDFs(RFE(StandardScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.3, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.001)), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Generation 23 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6768871952674014\tXGBClassifier(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-3\t0.6769630504269835\tXGBClassifier(CombineDFs(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.3, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.001)), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-4\t0.6769684175373312\tXGBClassifier(CombineDFs(RFE(StandardScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.3, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.001)), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 24 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6849813542610881\tRandomForestClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving best periodic pipeline to tpot_results.txt/pipeline_2019.01.23_21-25-16.py\n",
      "Skipped pipeline #262 due to time out. Continuing to the next pipeline.\n",
      "Generation 25 - Current Pareto front scores:\n",
      "-1\t0.6732483739643464\tXGBClassifier(CombineDFs(input_matrix, input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=3, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7000000000000001)\n",
      "-2\t0.6849813542610881\tRandomForestClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "Generation 26 - Current Pareto front scores:\n",
      "-1\t0.6769684175373312\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=16, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.6849813542610881\tRandomForestClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 27 - Current Pareto front scores:\n",
      "-1\t0.6769684175373312\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=16, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.6849813542610881\tRandomForestClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78\n",
      "Generation 28 - Current Pareto front scores:\n",
      "-1\t0.6769684175373312\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=16, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.6849813542610881\tRandomForestClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 29 - Current Pareto front scores:\n",
      "-1\t0.6769684175373312\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=16, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.6849813542610881\tRandomForestClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 30 - Current Pareto front scores:\n",
      "-1\t0.6769684175373312\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=16, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.6849813542610881\tRandomForestClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 31 - Current Pareto front scores:\n",
      "-1\t0.6769684175373312\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=16, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.6849813542610881\tRandomForestClassifier(CombineDFs(input_matrix, ZeroCount(input_matrix)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "\n",
      "The optimized pipeline was not improved after evaluating 5 more generations. Will end the optimization process.\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=10,\n",
       "        disable_update_check=False, early_stop=5, generations=40,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n",
       "        periodic_checkpoint_folder='tpot_results.txt', population_size=10,\n",
       "        random_state=25, scoring='accuracy', subsample=1.0, use_dask=False,\n",
       "        verbosity=3, warm_start=False)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(verbosity=3,cv=10,#config_dict=tpot_config,#max_time_mins=10,\n",
    " scoring='accuracy', \n",
    " random_state=25, \n",
    " periodic_checkpoint_folder='tpot_results.txt', \n",
    " n_jobs=-1, \n",
    " generations=40, \n",
    " population_size=10,\n",
    " early_stop = 5)\n",
    "tpot.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tpot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-c145c2bcbc0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_hat_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train set Accuracy for ensemble: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tpot' is not defined"
     ]
    }
   ],
   "source": [
    "y_hat_train = tpot.predict(x_train)\n",
    "train_acc = accuracy_score(y_train,y_hat_train)\n",
    "print(\"Train set Accuracy for ensemble: \" + str(train_acc))\n",
    "\n",
    "y_pred = tpot.predict(x_test)\n",
    "test_acc = accuracy_score(y_test,y_pred) \n",
    "print(\"Test Set Accuracy for ensemble: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
