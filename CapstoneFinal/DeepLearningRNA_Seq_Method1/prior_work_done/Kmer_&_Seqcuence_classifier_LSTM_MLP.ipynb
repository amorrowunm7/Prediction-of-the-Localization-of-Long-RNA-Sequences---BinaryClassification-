{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "#http://biopython.org/DIST/docs/tutorial/Tutorial.html\n",
    "#https://biopython.org/wiki/SeqIO\n",
    "from Bio import Seq, SeqIO\n",
    "\n",
    "#http://biopython.org/DIST/docs/api/Bio.Alphabet-module.html\n",
    "from Bio.Alphabet import generic_dna\n",
    "\n",
    "#https://docs.python.org/3/library/itertools.html\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, Conv1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, LeakyReLU, concatenate, MaxPool1D,GlobalMaxPool1D,add\n",
    "from keras.layers import Dense, Embedding, Input, Masking, Dropout, MaxPooling1D,Lambda, BatchNormalization\n",
    "from keras.layers import LSTM, TimeDistributed, AveragePooling1D, Flatten,Activation,ZeroPadding1D, UpSampling1D\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping,ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, ConvLSTM2D, Bidirectional,RepeatVector\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./NuclearCytosolLncRNAs_ALL_8mer_stride1_tokens.csv.count.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tttttttt</th>\n",
       "      <th>aaaaaaaa</th>\n",
       "      <th>ccaggctg</th>\n",
       "      <th>caggctgg</th>\n",
       "      <th>cacacaca</th>\n",
       "      <th>acacacac</th>\n",
       "      <th>cagcctcc</th>\n",
       "      <th>aaaataaa</th>\n",
       "      <th>aaataaaa</th>\n",
       "      <th>...</th>\n",
       "      <th>ctcaggag</th>\n",
       "      <th>ctgtcacc</th>\n",
       "      <th>ctccagaa</th>\n",
       "      <th>gaaatgaa</th>\n",
       "      <th>agccaggc</th>\n",
       "      <th>agatgaag</th>\n",
       "      <th>ctggagcc</th>\n",
       "      <th>tgcctggg</th>\n",
       "      <th>tggagaga</th>\n",
       "      <th>gagatggg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000446717.5</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000603052.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000608561.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000627496.2</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000423499.3</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     class  tttttttt  aaaaaaaa  ccaggctg  caggctgg  cacacaca  \\\n",
       "name                                                                           \n",
       "ENST00000446717.5  Nuclear         0         0         0         0         0   \n",
       "ENST00000603052.1  Nuclear         0         0         0         0         0   \n",
       "ENST00000608561.1  Nuclear         0         0         1         1         0   \n",
       "ENST00000627496.2  Nuclear         0         0         1         0         0   \n",
       "ENST00000423499.3  Nuclear        11         6         1         1         0   \n",
       "\n",
       "                   acacacac  cagcctcc  aaaataaa  aaataaaa    ...     ctcaggag  \\\n",
       "name                                                         ...                \n",
       "ENST00000446717.5         0         0         0         0    ...          0.0   \n",
       "ENST00000603052.1         0         0         0         0    ...          0.0   \n",
       "ENST00000608561.1         0         1         0         0    ...          0.0   \n",
       "ENST00000627496.2         0         0         1         1    ...          0.0   \n",
       "ENST00000423499.3         0         1         0         0    ...          1.0   \n",
       "\n",
       "                   ctgtcacc  ctccagaa  gaaatgaa  agccaggc  agatgaag  ctggagcc  \\\n",
       "name                                                                            \n",
       "ENST00000446717.5       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "ENST00000603052.1       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "ENST00000608561.1       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "ENST00000627496.2       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "ENST00000423499.3       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "\n",
       "                   tgcctggg  tggagaga  gagatggg  \n",
       "name                                             \n",
       "ENST00000446717.5       0.0       1.0       0.0  \n",
       "ENST00000603052.1       0.0       0.0       0.0  \n",
       "ENST00000608561.1       0.0       0.0       0.0  \n",
       "ENST00000627496.2       0.0       0.0       0.0  \n",
       "ENST00000423499.3       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_2_csv(path):\n",
    "    #Reading a fasta database as a panda dataframe\n",
    "    reads=[]\n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        id_ = record.description.split(' ')[0]\n",
    "        seq_ = str(record.seq)\n",
    "        reads.append([id_,seq_,len(record.seq)])    \n",
    "    df = pd.DataFrame(reads,columns=['id','seq','len'])\n",
    "    # Applying encoding\n",
    "    #df['encoded'] = df['seq'].apply(encode_nu)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_df = fasta_2_csv('./lncRNA_amanda.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000473358.1</td>\n",
       "      <td>GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000461467.1</td>\n",
       "      <td>GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000495576.1</td>\n",
       "      <td>TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...</td>\n",
       "      <td>1319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000471248.1</td>\n",
       "      <td>GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000484859.1</td>\n",
       "      <td>GCCATTTCTTTTTTTTCTTTTTTTTTTTAAGATAAGAGTCTTTCTC...</td>\n",
       "      <td>4860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                                seq   len\n",
       "0  ENST00000473358.1  GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...   712\n",
       "1  ENST00000461467.1  GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...   590\n",
       "2  ENST00000495576.1  TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...  1319\n",
       "3  ENST00000471248.1  GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...   629\n",
       "4  ENST00000484859.1  GCCATTTCTTTTTTTTCTTTTTTTTTTTAAGATAAGAGTCTTTCTC...  4860"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the classes from df to fasta_df\n",
    "fasta_df.index = fasta_df['id']\n",
    "df['id'] = df.index\n",
    "fasta_df['class'] = fasta_df['id'].map(df.set_index('id')['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any rows with NAin the class \n",
    "fasta_df = fasta_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>len</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000466557.6</th>\n",
       "      <td>ENST00000466557.6</td>\n",
       "      <td>ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...</td>\n",
       "      <td>1301</td>\n",
       "      <td>Nuclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000450983.1</th>\n",
       "      <td>ENST00000450983.1</td>\n",
       "      <td>TACGGCAGCTTTAGGGAGGTGCTCTGAGACCCGAAACTAGACTCGA...</td>\n",
       "      <td>607</td>\n",
       "      <td>Cytosol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000623070.3</th>\n",
       "      <td>ENST00000623070.3</td>\n",
       "      <td>GTGTCTGCGTCGGGTTCTGTTGGAGTGCGTTCGGTGCGCCGTGGGT...</td>\n",
       "      <td>494</td>\n",
       "      <td>Nuclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000425657.1</th>\n",
       "      <td>ENST00000425657.1</td>\n",
       "      <td>GTCTCAGCCAGCAGGGTTGCCCAGTGCCCCTTGTCACCCCCCGAGC...</td>\n",
       "      <td>845</td>\n",
       "      <td>Nuclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000606034.1</th>\n",
       "      <td>ENST00000606034.1</td>\n",
       "      <td>TCCCAACTGGGGCCGAGTTCACCAGGGCCCACGGGAGGCGAGCGAG...</td>\n",
       "      <td>2086</td>\n",
       "      <td>Nuclear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "id                                     \n",
       "ENST00000466557.6  ENST00000466557.6   \n",
       "ENST00000450983.1  ENST00000450983.1   \n",
       "ENST00000623070.3  ENST00000623070.3   \n",
       "ENST00000425657.1  ENST00000425657.1   \n",
       "ENST00000606034.1  ENST00000606034.1   \n",
       "\n",
       "                                                                 seq   len  \\\n",
       "id                                                                           \n",
       "ENST00000466557.6  ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...  1301   \n",
       "ENST00000450983.1  TACGGCAGCTTTAGGGAGGTGCTCTGAGACCCGAAACTAGACTCGA...   607   \n",
       "ENST00000623070.3  GTGTCTGCGTCGGGTTCTGTTGGAGTGCGTTCGGTGCGCCGTGGGT...   494   \n",
       "ENST00000425657.1  GTCTCAGCCAGCAGGGTTGCCCAGTGCCCCTTGTCACCCCCCGAGC...   845   \n",
       "ENST00000606034.1  TCCCAACTGGGGCCGAGTTCACCAGGGCCCACGGGAGGCGAGCGAG...  2086   \n",
       "\n",
       "                     class  \n",
       "id                          \n",
       "ENST00000466557.6  Nuclear  \n",
       "ENST00000450983.1  Cytosol  \n",
       "ENST00000623070.3  Nuclear  \n",
       "ENST00000425657.1  Nuclear  \n",
       "ENST00000606034.1  Nuclear  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 258)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 4)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the classes to 0 and 1\n",
    "fasta_df['class'] = pd.factorize(fasta_df['class'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmer Based Modeling\n",
    "\n",
    "\n",
    "https://www.cs.helsinki.fi/u/tpkarkka/teach/16-17/MAiB/kMerStatistics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strtify sample to train , test and validation data\n",
    "X=fasta_df.drop(columns=['class','id','len'])\n",
    "y=fasta_df.loc[:,'class']\n",
    "train, test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=15)\n",
    "#train, valid, y_train, y_valid = train_test_split(train,y_train,test_size=0.15, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "KMER_SIZE = 7\n",
    "counter = CountVectorizer(input='content',#vocabulary=int_to_word,\n",
    "                          analyzer='char',ngram_range=(KMER_SIZE,KMER_SIZE),\n",
    "                          lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.fit(train.seq.values.tolist())\n",
    "X_train = counter.transform(train.seq.values.tolist())\n",
    "X_test = counter.transform(test.seq.values.tolist())\n",
    "#X_valid = counter.transform(valid.seq.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.decomposition import SparsePCA,PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Bidirectional,Embedding, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Conv1D, AveragePooling1D, concatenate,GlobalMaxPool1D,LSTM\n",
    "from keras.layers import GRU, Flatten, TimeDistributed, SpatialDropout1D, GlobalMaxPooling1D, Masking,Activation,GRU\n",
    "from keras.optimizers import rmsprop, adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pipeline is a list of preprocessing steps in one command\n",
    "# first we will remove high correlated features\n",
    "# then we will apply scaling as neural network needs values frm 0 to 1\n",
    "pipeline = Pipeline([#('corr-removal',VarianceThreshold(0.8)),\n",
    "                     #('selector',SelectKBest(chi2,500)),\n",
    "                     #('pca',PCA()),\n",
    "                     ('scaler',MaxAbsScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the pipeline\n",
    "x_train = pipeline.fit_transform(X_train.toarray(),y_train)\n",
    "# notice that the pipeline learn the best parameters fro the tarin data only to apply to test ana valiadation data\n",
    "x_test = pipeline.transform(X_test.toarray())\n",
    "#x_valid = pipeline.transform(X_valid.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 16384)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing some simple ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Hyperparameter search\n",
    "# I didn't used any internal valiadtion as the next algorithms uses cross validation cv =10\n",
    "cv = 5\n",
    "n_iter = 5\n",
    "n_jobs= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6075"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "parameters = {'n_estimators':(10,20,30,40,50,60),\n",
    "              'max_features': (10,20,8,5,'auto'),\n",
    "              'max_depth':(5,20,50,100,500,250,20,10)}\n",
    "clf= RandomizedSearchCV(clf, parameters,cv=cv,n_jobs=n_jobs,n_iter=n_iter)\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_test)\n",
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biohacker2/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.645"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "parameters = {'C':(0.3,0.5,0.55,0.6,0.75),\n",
    "              'tol': (0.0002,0.0004,.0005)}\n",
    "#clf = RandomizedSearchCV(clf, parameters,cv=cv,n_jobs=n_jobs,n_iter=n_iter)\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_test)\n",
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tpot apply genetic algorithm for hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes alot of time\n",
    "# you will need to install tpot first pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4561ffdf7d4820855dd88040bb3bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=510), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6310232577444432\n",
      "Generation 2 - Current best internal CV score: 0.6310232577444432\n",
      "Generation 3 - Current best internal CV score: 0.6310232577444432\n",
      "Generation 4 - Current best internal CV score: 0.6310232577444432\n",
      "Generation 5 - Current best internal CV score: 0.6310232577444432\n",
      "\n",
      "The optimized pipeline was not improved after evaluating 5 more generations. Will end the optimization process.\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: LogisticRegression(input_matrix, C=0.1, dual=False, penalty=l2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=10,\n",
       "        disable_update_check=False, early_stop=5, generations=50,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n",
       "        periodic_checkpoint_folder='tpot_results.txt', population_size=10,\n",
       "        random_state=0, scoring='accuracy', subsample=1.0, use_dask=False,\n",
       "        verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(verbosity=2,cv=10,#config_dict=tpot_config,#max_time_mins=10,\n",
    " scoring='accuracy', \n",
    " random_state=0, \n",
    " periodic_checkpoint_folder='tpot_results.txt', \n",
    " n_jobs=-1, \n",
    " generations=50, \n",
    " population_size=10,\n",
    " early_stop = 5)\n",
    "tpot.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Accuracy for ensemble: 0.99812382739212\n",
      "Test Set Accuracy for ensemble: 0.66\n"
     ]
    }
   ],
   "source": [
    "y_hat_train = tpot.predict(x_train)\n",
    "train_acc = accuracy_score(y_train,y_hat_train)\n",
    "print(\"Train set Accuracy for ensemble: \" + str(train_acc))\n",
    "\n",
    "y_pred = tpot.predict(x_test)\n",
    "test_acc = accuracy_score(y_test,y_pred) \n",
    "print(\"Test Set Accuracy for ensemble: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sequence of charaters to a sequence of kmers\n",
    "def seq_to_kmer(seq):\n",
    "    return ' '.join([seq[i:i+KMER_SIZE] for i in range(len(seq)-KMER_SIZE)]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER_SIZE= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7faee8be4240>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## As some sequence are very long which may cause overfitting for small sequences\n",
    "# We will determine a cutoff based on standard deviation\n",
    "fasta_df.len.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = int(fasta_df.len.std()*2)\n",
    "fasta_df['seq_cutted'] = fasta_df['seq'].apply(lambda x: x[0:max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_df['kmers'] = fasta_df['seq_cutted'].apply(lambda x: seq_to_kmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>len</th>\n",
       "      <th>class</th>\n",
       "      <th>seq_cutted</th>\n",
       "      <th>kmers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000466557.6</th>\n",
       "      <td>ENST00000466557.6</td>\n",
       "      <td>ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...</td>\n",
       "      <td>1301</td>\n",
       "      <td>0</td>\n",
       "      <td>ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...</td>\n",
       "      <td>atgatg tgatga gatgat atgatt tgatta gattat atta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000450983.1</th>\n",
       "      <td>ENST00000450983.1</td>\n",
       "      <td>TACGGCAGCTTTAGGGAGGTGCTCTGAGACCCGAAACTAGACTCGA...</td>\n",
       "      <td>607</td>\n",
       "      <td>1</td>\n",
       "      <td>TACGGCAGCTTTAGGGAGGTGCTCTGAGACCCGAAACTAGACTCGA...</td>\n",
       "      <td>tacggc acggca cggcag ggcagc gcagct cagctt agct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000623070.3</th>\n",
       "      <td>ENST00000623070.3</td>\n",
       "      <td>GTGTCTGCGTCGGGTTCTGTTGGAGTGCGTTCGGTGCGCCGTGGGT...</td>\n",
       "      <td>494</td>\n",
       "      <td>0</td>\n",
       "      <td>GTGTCTGCGTCGGGTTCTGTTGGAGTGCGTTCGGTGCGCCGTGGGT...</td>\n",
       "      <td>gtgtct tgtctg gtctgc tctgcg ctgcgt tgcgtc gcgt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000425657.1</th>\n",
       "      <td>ENST00000425657.1</td>\n",
       "      <td>GTCTCAGCCAGCAGGGTTGCCCAGTGCCCCTTGTCACCCCCCGAGC...</td>\n",
       "      <td>845</td>\n",
       "      <td>0</td>\n",
       "      <td>GTCTCAGCCAGCAGGGTTGCCCAGTGCCCCTTGTCACCCCCCGAGC...</td>\n",
       "      <td>gtctca tctcag ctcagc tcagcc cagcca agccag gcca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000606034.1</th>\n",
       "      <td>ENST00000606034.1</td>\n",
       "      <td>TCCCAACTGGGGCCGAGTTCACCAGGGCCCACGGGAGGCGAGCGAG...</td>\n",
       "      <td>2086</td>\n",
       "      <td>0</td>\n",
       "      <td>TCCCAACTGGGGCCGAGTTCACCAGGGCCCACGGGAGGCGAGCGAG...</td>\n",
       "      <td>tcccaa cccaac ccaact caactg aactgg actggg ctgg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "id                                     \n",
       "ENST00000466557.6  ENST00000466557.6   \n",
       "ENST00000450983.1  ENST00000450983.1   \n",
       "ENST00000623070.3  ENST00000623070.3   \n",
       "ENST00000425657.1  ENST00000425657.1   \n",
       "ENST00000606034.1  ENST00000606034.1   \n",
       "\n",
       "                                                                 seq   len  \\\n",
       "id                                                                           \n",
       "ENST00000466557.6  ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...  1301   \n",
       "ENST00000450983.1  TACGGCAGCTTTAGGGAGGTGCTCTGAGACCCGAAACTAGACTCGA...   607   \n",
       "ENST00000623070.3  GTGTCTGCGTCGGGTTCTGTTGGAGTGCGTTCGGTGCGCCGTGGGT...   494   \n",
       "ENST00000425657.1  GTCTCAGCCAGCAGGGTTGCCCAGTGCCCCTTGTCACCCCCCGAGC...   845   \n",
       "ENST00000606034.1  TCCCAACTGGGGCCGAGTTCACCAGGGCCCACGGGAGGCGAGCGAG...  2086   \n",
       "\n",
       "                   class                                         seq_cutted  \\\n",
       "id                                                                            \n",
       "ENST00000466557.6      0  ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...   \n",
       "ENST00000450983.1      1  TACGGCAGCTTTAGGGAGGTGCTCTGAGACCCGAAACTAGACTCGA...   \n",
       "ENST00000623070.3      0  GTGTCTGCGTCGGGTTCTGTTGGAGTGCGTTCGGTGCGCCGTGGGT...   \n",
       "ENST00000425657.1      0  GTCTCAGCCAGCAGGGTTGCCCAGTGCCCCTTGTCACCCCCCGAGC...   \n",
       "ENST00000606034.1      0  TCCCAACTGGGGCCGAGTTCACCAGGGCCCACGGGAGGCGAGCGAG...   \n",
       "\n",
       "                                                               kmers  \n",
       "id                                                                    \n",
       "ENST00000466557.6  atgatg tgatga gatgat atgatt tgatta gattat atta...  \n",
       "ENST00000450983.1  tacggc acggca cggcag ggcagc gcagct cagctt agct...  \n",
       "ENST00000623070.3  gtgtct tgtctg gtctgc tctgcg ctgcgt tgcgtc gcgt...  \n",
       "ENST00000425657.1  gtctca tctcag ctcagc tcagcc cagcca agccag gcca...  \n",
       "ENST00000606034.1  tcccaa cccaac ccaact caactg aactgg actggg ctgg...  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strtify sample to train , test and validation data\n",
    "# Validation data is important for neural network backpropagation\n",
    "X=fasta_df.drop(columns=['class','id','len','seq','seq_cutted'])\n",
    "y=fasta_df.loc[:,'class']\n",
    "train, test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=15)\n",
    "# The test data is the same but we take the validatin data from the train\n",
    "train, val, y_train, y_val = train_test_split(train,y_train,test_size=0.15, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "tokenizer = Tokenizer(split=' ')\n",
    "tokenizer.fit_on_texts(train.values.tolist())\n",
    "X_train = tokenizer.texts_to_sequences(train.values.tolist())\n",
    "X_test = tokenizer.texts_to_sequences(test.values.tolist())\n",
    "X_val = tokenizer.texts_to_sequences(val.values.tolist())\n",
    "# Padding the sequence of vectors with zeros\n",
    "X_train = pad_sequences(X_train,maxlen = max_len)\n",
    "X_test = pad_sequences(X_test,maxlen = max_len )\n",
    "X_val = pad_sequences(X_val,maxlen = max_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1359, 2811)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = X_train.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the output to categorical classes\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)\n",
    "Y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate class weights\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  np.unique(Y_train.argmax(-1)),\n",
    "                                                  Y_train.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_classifier(bidirectional, add_attention,trained_embedding):\n",
    "    inp1 = Input(shape=(X_train.shape[1],),dtype='uint16')    \n",
    "    if trained_embedding:\n",
    "        emb = embedding\n",
    "        main = emb(inp1)\n",
    "    else:\n",
    "        main = Embedding(max_features,120)(inp1)\n",
    "\n",
    "    if bidirectional ==True and add_attention ==False:\n",
    "        main = Bidirectional(LSTM(16, return_sequences=False))(main)\n",
    "     \n",
    "    elif add_attention ==True:\n",
    "        main = Bidirectional(LSTM(16, return_sequences=True))(main)\n",
    "        main = Attention_layer()(main)     \n",
    "    else:\n",
    "        main = LSTM(16, return_sequences=False)(main)\n",
    "        \n",
    "    main = Activation('relu')(main)\n",
    "    main = BatchNormalization()(main)\n",
    "    main = Dropout(0.5)(main)\n",
    "    main = Dense(128)(main) \n",
    "    main = Dropout(0.5)(main)\n",
    "    main = Dense(64)(main) \n",
    "    main = Dropout(0.5)(main)\n",
    "    main = BatchNormalization()(main)\n",
    "    out = Dense(2,activation='sigmoid')(main)\n",
    "    \n",
    "    model = Model(inputs=[inp1], outputs=[out])\n",
    "    optimizer = adam(0.001,amsgrad=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention Implementation in Keras by https://github.com/BITDM/bitdm.github.io\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class Attention_layer(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (LSTM/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        super(Attention_layer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "\n",
    "        a = K.exp(uit)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 88s - loss: 0.8487 - acc: 0.4985 - val_loss: 0.6986 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 75s - loss: 0.7966 - acc: 0.5011 - val_loss: 0.7800 - val_acc: 0.4167\n",
      "Epoch 3/50\n",
      " - 84s - loss: 0.6822 - acc: 0.6100 - val_loss: 0.7786 - val_acc: 0.4167\n",
      "Epoch 4/50\n",
      " - 80s - loss: 0.5403 - acc: 0.7152 - val_loss: 0.8414 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 82s - loss: 0.4484 - acc: 0.7896 - val_loss: 0.8447 - val_acc: 0.4167\n",
      "Epoch 6/50\n",
      " - 86s - loss: 0.3862 - acc: 0.8322 - val_loss: 0.8389 - val_acc: 0.4167\n",
      "400/400 [==============================] - 5s 13ms/step\n",
      "LSTM Model Accuracy : 0.495\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=False,add_attention=False,trained_embedding=False)\n",
    "checkpoint = ModelCheckpoint(filepath='LSTM.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "#model.load_weights('2input_BiLSTM.hdfs')\n",
    "\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('LSTM.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 165s - loss: 0.8817 - acc: 0.4941 - val_loss: 0.6991 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 196s - loss: 0.8288 - acc: 0.5070 - val_loss: 0.7165 - val_acc: 0.4167\n",
      "Epoch 3/50\n",
      " - 136s - loss: 0.7776 - acc: 0.5298 - val_loss: 0.6819 - val_acc: 0.5833\n",
      "Epoch 4/50\n",
      " - 152s - loss: 0.6709 - acc: 0.6229 - val_loss: 0.6989 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 147s - loss: 0.4615 - acc: 0.7859 - val_loss: 0.6911 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      " - 175s - loss: 0.2955 - acc: 0.8830 - val_loss: 0.7912 - val_acc: 0.4167\n",
      "Epoch 7/50\n",
      " - 175s - loss: 0.2335 - acc: 0.9227 - val_loss: 0.7900 - val_acc: 0.4167\n",
      "Epoch 8/50\n",
      " - 156s - loss: 0.1968 - acc: 0.9393 - val_loss: 0.7258 - val_acc: 0.4167\n",
      "400/400 [==============================] - 9s 23ms/step\n",
      "LSTM Model Accuracy : 0.505\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=True,add_attention=False,trained_embedding=False)\n",
    "checkpoint = ModelCheckpoint(filepath='Bi_LSTM.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('Bi_LSTM.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 159s - loss: 0.7223 - acc: 0.4937 - val_loss: 0.6974 - val_acc: 0.5833\n",
      "Epoch 2/50\n",
      " - 169s - loss: 0.7208 - acc: 0.4812 - val_loss: 0.7334 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      " - 176s - loss: 0.7098 - acc: 0.5202 - val_loss: 1.6002 - val_acc: 0.5833\n",
      "Epoch 4/50\n",
      " - 184s - loss: 0.6951 - acc: 0.5298 - val_loss: 6.6793 - val_acc: 0.5833\n",
      "Epoch 5/50\n",
      " - 162s - loss: 0.6512 - acc: 0.6207 - val_loss: 6.6793 - val_acc: 0.5833\n",
      "Epoch 6/50\n",
      " - 151s - loss: 0.4846 - acc: 0.8079 - val_loss: 6.6793 - val_acc: 0.5833\n",
      "400/400 [==============================] - 9s 23ms/step\n",
      "Attention_Bi_LSTM Model Accuracy : 0.505\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM + Attention Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=True,add_attention=True,trained_embedding=False)\n",
    "checkpoint = ModelCheckpoint(filepath='Attention_Bi_LSTM.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('Attention_Bi_LSTM.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('Attention_Bi_LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid the MLP neural network \n",
    "def build_mlp_classifier():\n",
    "    #the input must have the number of features\n",
    "    inp1 = Input(shape=(X_train.shape[1],),dtype='float')\n",
    "    emb =Embedding(max_features,120)(inp1)\n",
    "    main = Dense(64)(emb)\n",
    "    # Dropouts are important t prevent the overfitting\n",
    "    main = Dropout(0.5)(main)\n",
    "    # Batch normalization allow faster convergence\n",
    "    main = BatchNormalization()(main)\n",
    "    main = Dense(64)(main)  \n",
    "    main = Dropout(0.5)(main)\n",
    "    main = BatchNormalization()(main)\n",
    "    main = GlobalMaxPooling1D()(main)\n",
    "    main = Dense(32)(main)  \n",
    "    #main = Dropout(0.5)(main)\n",
    "    #main = Dense(256)(main)  \n",
    "    #main = Dropout(0.5)(main)\n",
    "    # Simoid function is a must in binary classification\n",
    "    out = Dense(2,activation='sigmoid')(main)\n",
    "    model = Model(inputs=[inp1], outputs=[out])\n",
    "    # Slower leraning rate is important in small dataset < 0.001\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 37s - loss: 0.8657 - acc: 0.5118 - val_loss: 0.6866 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      " - 34s - loss: 0.7531 - acc: 0.4930 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      " - 33s - loss: 0.7569 - acc: 0.4941 - val_loss: 0.6919 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      " - 34s - loss: 0.7368 - acc: 0.4982 - val_loss: 0.6899 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      " - 33s - loss: 0.7419 - acc: 0.4923 - val_loss: 0.6894 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      " - 34s - loss: 0.7286 - acc: 0.5132 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 7/50\n",
      " - 33s - loss: 0.7270 - acc: 0.5151 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 8/50\n",
      " - 34s - loss: 0.7232 - acc: 0.5132 - val_loss: 0.6866 - val_acc: 0.5833\n",
      "Epoch 9/50\n",
      " - 35s - loss: 0.7314 - acc: 0.5184 - val_loss: 0.6877 - val_acc: 0.5833\n",
      "Epoch 10/50\n",
      " - 36s - loss: 0.7186 - acc: 0.5258 - val_loss: 0.6890 - val_acc: 0.5833\n",
      "Epoch 11/50\n",
      " - 36s - loss: 0.7115 - acc: 0.5397 - val_loss: 0.6897 - val_acc: 0.5833\n",
      "Epoch 12/50\n",
      " - 35s - loss: 0.7168 - acc: 0.5140 - val_loss: 0.6903 - val_acc: 0.5833\n",
      "400/400 [==============================] - 2s 6ms/step\n",
      "MLP_sequence_based Model Accuracy : 0.505\n"
     ]
    }
   ],
   "source": [
    "# MLP_sequence_based\n",
    "batch_size = 32\n",
    "model = build_mlp_classifier()\n",
    "checkpoint = ModelCheckpoint(filepath='MLP_sequence_based.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('MLP_sequence_based.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('MLP_sequence_based Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Google W2V will take a lot of memory\n",
    "from gensim.models import  KeyedVectors,Word2Vec \n",
    "import multiprocessing\n",
    "\n",
    "# Gensim Word2Vec model\n",
    "def get_embedding(x_array,tokenizer,max_features):\n",
    "    # Function for getting word2ve ang keras embedding layer\n",
    "    df = pd.DataFrame()\n",
    "    df['x'] = tokenizer.sequences_to_texts(x_array)\n",
    "    corpus = df['x'].apply(lambda x: x.split(' '))\n",
    "\n",
    "    # Create Word2Vec\n",
    "    word2vec = Word2Vec(sentences=corpus,\n",
    "                        #size=120,#max_vocab_size=max_features,\n",
    "                        hs=1,\n",
    "                        seed=1,\n",
    "                        workers=multiprocessing.cpu_count())\n",
    "    embedding = word2vec.wv.get_keras_embedding(train_embeddings=False)\n",
    "    return word2vec, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Getting the embedding from the word2vec\n",
    "word2vec,embedding = get_embedding(X_train,tokenizer,max_features)\n",
    "word2vec.save(\"text_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 84s - loss: 0.8494 - acc: 0.5007 - val_loss: 0.7347 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 86s - loss: 0.8169 - acc: 0.4982 - val_loss: 0.6960 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      " - 85s - loss: 0.7901 - acc: 0.5022 - val_loss: 0.6977 - val_acc: 0.4167\n",
      "Epoch 4/50\n",
      " - 73s - loss: 0.7739 - acc: 0.5022 - val_loss: 0.7006 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 83s - loss: 0.7414 - acc: 0.5269 - val_loss: 0.7021 - val_acc: 0.4167\n",
      "Epoch 6/50\n",
      " - 72s - loss: 0.7482 - acc: 0.5166 - val_loss: 0.7002 - val_acc: 0.4167\n",
      "Epoch 7/50\n",
      " - 79s - loss: 0.7509 - acc: 0.5022 - val_loss: 0.6993 - val_acc: 0.4167\n",
      "400/400 [==============================] - 7s 17ms/step\n",
      "LSTM Model Accuracy : 0.5\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=False,add_attention=False,trained_embedding=True)\n",
    "checkpoint = ModelCheckpoint(filepath='LSTM_w2v.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "#model.load_weights('2input_BiLSTM.hdfs')\n",
    "\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('LSTM_w2v.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 118s - loss: 0.8716 - acc: 0.4930 - val_loss: 0.6953 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 113s - loss: 0.8084 - acc: 0.4956 - val_loss: 0.7081 - val_acc: 0.4167\n",
      "Epoch 3/50\n",
      " - 112s - loss: 0.7742 - acc: 0.5081 - val_loss: 0.7092 - val_acc: 0.4167\n",
      "Epoch 4/50\n",
      " - 114s - loss: 0.7780 - acc: 0.5088 - val_loss: 0.7041 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 116s - loss: 0.7674 - acc: 0.5140 - val_loss: 0.7057 - val_acc: 0.4167\n",
      "Epoch 6/50\n",
      " - 125s - loss: 0.7561 - acc: 0.5283 - val_loss: 0.7062 - val_acc: 0.4167\n",
      "400/400 [==============================] - 8s 20ms/step\n",
      "LSTM Model Accuracy : 0.495\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=True,add_attention=False,trained_embedding=True)\n",
    "checkpoint = ModelCheckpoint(filepath='Bi_LSTM_w2v.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('Bi_LSTM_w2v.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 130s - loss: 0.7367 - acc: 0.4960 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 115s - loss: 0.7188 - acc: 0.5092 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 3/50\n",
      " - 99s - loss: 0.7206 - acc: 0.5136 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 4/50\n",
      " - 98s - loss: 0.7124 - acc: 0.5125 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 100s - loss: 0.7077 - acc: 0.4956 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 6/50\n",
      " - 97s - loss: 0.7010 - acc: 0.5022 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "400/400 [==============================] - 8s 19ms/step\n",
      "Attention_Bi_LSTM Model Accuracy : 0.495\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM + Attention Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=True,add_attention=True,trained_embedding=True)\n",
    "checkpoint = ModelCheckpoint(filepath='Attention_Bi_LSTM_w2v.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('Attention_Bi_LSTM_w2v.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('Attention_Bi_LSTM Model Accuracy : '+ str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
