{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import Bio\n",
    "from Bio import Seq, SeqIO\n",
    "from Bio.Alphabet import generic_dna\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, Conv1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, LeakyReLU, concatenate, MaxPool1D,GlobalMaxPool1D,add\n",
    "from keras.layers import Dense, Embedding, Input, Masking, Dropout, MaxPooling1D,Lambda, BatchNormalization\n",
    "from keras.layers import LSTM, TimeDistributed, AveragePooling1D, Flatten,Activation,ZeroPadding1D, UpSampling1D\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping,ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, ConvLSTM2D, Bidirectional,RepeatVector\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./lincRNA.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Log 2 Fol-Change</th>\n",
       "      <th>transcript_length</th>\n",
       "      <th>GC_content (%)</th>\n",
       "      <th>sense</th>\n",
       "      <th>chromosomE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000642138.1</th>\n",
       "      <td>Cytosol</td>\n",
       "      <td>-0.147716</td>\n",
       "      <td>673</td>\n",
       "      <td>0.531947</td>\n",
       "      <td>lincRNA</td>\n",
       "      <td>chromosome_name1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000642115.2</th>\n",
       "      <td>Cytosol</td>\n",
       "      <td>-0.381503</td>\n",
       "      <td>2962</td>\n",
       "      <td>0.423025</td>\n",
       "      <td>lincRNA</td>\n",
       "      <td>chromosome_name5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000642102.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>4.914269</td>\n",
       "      <td>1351</td>\n",
       "      <td>0.348631</td>\n",
       "      <td>lincRNA</td>\n",
       "      <td>chromosome_name8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000642076.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>2.818342</td>\n",
       "      <td>165</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>lincRNA</td>\n",
       "      <td>chromosome_name8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000642031.1</th>\n",
       "      <td>Nuclear</td>\n",
       "      <td>2.989621</td>\n",
       "      <td>997</td>\n",
       "      <td>0.382146</td>\n",
       "      <td>lincRNA</td>\n",
       "      <td>chromosome_name1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     class  Log 2 Fol-Change  transcript_length  \\\n",
       "name                                                              \n",
       "ENST00000642138.1  Cytosol         -0.147716                673   \n",
       "ENST00000642115.2  Cytosol         -0.381503               2962   \n",
       "ENST00000642102.1  Nuclear          4.914269               1351   \n",
       "ENST00000642076.1  Nuclear          2.818342                165   \n",
       "ENST00000642031.1  Nuclear          2.989621                997   \n",
       "\n",
       "                   GC_content (%)    sense        chromosomE  \n",
       "name                                                          \n",
       "ENST00000642138.1        0.531947  lincRNA  chromosome_name1  \n",
       "ENST00000642115.2        0.423025  lincRNA  chromosome_name5  \n",
       "ENST00000642102.1        0.348631  lincRNA  chromosome_name8  \n",
       "ENST00000642076.1        0.484848  lincRNA  chromosome_name8  \n",
       "ENST00000642031.1        0.382146  lincRNA  chromosome_name1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNA_2_csv(path):\n",
    "    #Reading  database as a panda dataframe\n",
    "    reads=[]\n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        id_ = record.description.split(' ')[0]\n",
    "        seq_ = str(record.seq)\n",
    "        reads.append([id_,seq_,len(record.seq)])    \n",
    "    df = pd.DataFrame(reads,columns=['id','seq','len'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA_df = RNA_2_csv('./lncRNA.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000473358.1</td>\n",
       "      <td>GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000461467.1</td>\n",
       "      <td>GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000495576.1</td>\n",
       "      <td>TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...</td>\n",
       "      <td>1319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000471248.1</td>\n",
       "      <td>GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000484859.1</td>\n",
       "      <td>GCCATTTCTTTTTTTTCTTTTTTTTTTTAAGATAAGAGTCTTTCTC...</td>\n",
       "      <td>4860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                                seq   len\n",
       "0  ENST00000473358.1  GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...   712\n",
       "1  ENST00000461467.1  GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...   590\n",
       "2  ENST00000495576.1  TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...  1319\n",
       "3  ENST00000471248.1  GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...   629\n",
       "4  ENST00000484859.1  GCCATTTCTTTTTTTTCTTTTTTTTTTTAAGATAAGAGTCTTTCTC...  4860"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the classes from df to RNA_df\n",
    "RNA_df.index = RNA_df['id']\n",
    "df['id'] = df.index\n",
    "RNA_df['class'] = RNA_df['id'].map(df.set_index('id')['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with NAin the class \n",
    "RNA_df = RNA_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>len</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000473358.1</th>\n",
       "      <td>ENST00000473358.1</td>\n",
       "      <td>GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...</td>\n",
       "      <td>712</td>\n",
       "      <td>Nuclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000461467.1</th>\n",
       "      <td>ENST00000461467.1</td>\n",
       "      <td>GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...</td>\n",
       "      <td>590</td>\n",
       "      <td>Nuclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000495576.1</th>\n",
       "      <td>ENST00000495576.1</td>\n",
       "      <td>TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...</td>\n",
       "      <td>1319</td>\n",
       "      <td>Cytosol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000471248.1</th>\n",
       "      <td>ENST00000471248.1</td>\n",
       "      <td>GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...</td>\n",
       "      <td>629</td>\n",
       "      <td>Nuclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000466557.6</th>\n",
       "      <td>ENST00000466557.6</td>\n",
       "      <td>ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...</td>\n",
       "      <td>1301</td>\n",
       "      <td>Nuclear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "id                                     \n",
       "ENST00000473358.1  ENST00000473358.1   \n",
       "ENST00000461467.1  ENST00000461467.1   \n",
       "ENST00000495576.1  ENST00000495576.1   \n",
       "ENST00000471248.1  ENST00000471248.1   \n",
       "ENST00000466557.6  ENST00000466557.6   \n",
       "\n",
       "                                                                 seq   len  \\\n",
       "id                                                                           \n",
       "ENST00000473358.1  GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...   712   \n",
       "ENST00000461467.1  GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...   590   \n",
       "ENST00000495576.1  TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...  1319   \n",
       "ENST00000471248.1  GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...   629   \n",
       "ENST00000466557.6  ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...  1301   \n",
       "\n",
       "                     class  \n",
       "id                          \n",
       "ENST00000473358.1  Nuclear  \n",
       "ENST00000461467.1  Nuclear  \n",
       "ENST00000495576.1  Cytosol  \n",
       "ENST00000471248.1  Nuclear  \n",
       "ENST00000466557.6  Nuclear  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3975, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3975, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNA_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the classes to 0 and 1\n",
    "RNA_df['class'] = pd.factorize(RNA_df['class'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmer Based Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to  sample to train , test and validation data\n",
    "X=RNA_df.drop(columns=['class','id','len'])\n",
    "y=RNA_df.loc[:,'class']\n",
    "train, test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=15)\n",
    "#train, valid, y_train, y_valid = train_test_split(train,y_train,test_size=0.15, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "KMER_SIZE = 8\n",
    "counter = CountVectorizer(input='content',#vocabulary=int_to_word,\n",
    "                          analyzer='char',ngram_range=(KMER_SIZE,KMER_SIZE),\n",
    "                          lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.fit(train.seq.values.tolist())\n",
    "X_train = counter.transform(train.seq.values.tolist())\n",
    "X_test = counter.transform(test.seq.values.tolist())\n",
    "#X_valid = counter.transform(valid.seq.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.decomposition import SparsePCA,PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Bidirectional,Embedding, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Conv1D, AveragePooling1D, concatenate,GlobalMaxPool1D,LSTM\n",
    "from keras.layers import GRU, Flatten, TimeDistributed, SpatialDropout1D, GlobalMaxPooling1D, Masking,Activation,GRU\n",
    "from keras.optimizers import rmsprop, adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([#('corr-removal',VarianceThreshold(0.8)),\n",
    "                     #('selector',SelectKBest(chi2,500)),\n",
    "                     #('pca',PCA()),\n",
    "                     ('scaler',MaxAbsScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the pipeline\n",
    "x_train = pipeline.fit_transform(X_train.toarray(),y_train)\n",
    "x_test = pipeline.transform(X_test.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3180, 16384)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing some ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Hyperparameter search\n",
    "# I didn't used any internal valiadtion since the next algorithms uses cross validation cv =10\n",
    "cv = 5\n",
    "n_iter = 5\n",
    "n_jobs= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6314465408805031"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "parameters = {'n_estimators':(10,20,30,40,50,60),\n",
    "              'max_features': (10,20,8,5,'auto'),\n",
    "              'max_depth':(5,20,50,100,500,250,20,10)}\n",
    "clf= RandomizedSearchCV(clf, parameters,cv=cv,n_jobs=n_jobs,n_iter=n_iter)\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_test)\n",
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amandam1/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6779874213836478"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "parameters = {'C':(0.3,0.5,0.55,0.6,0.75),\n",
    "              'tol': (0.0002,0.0004,.0005)}\n",
    "#clf = RandomizedSearchCV(clf, parameters,cv=cv,n_jobs=n_jobs,n_iter=n_iter)\n",
    "clf.fit(x_train,y_train)\n",
    "pred = clf.predict(x_test)\n",
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sequence of charaters to a sequence of kmers\n",
    "def seq_to_kmer(seq):\n",
    "    return ' '.join([seq[i:i+KMER_SIZE] for i in range(len(seq)-KMER_SIZE)]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER_SIZE= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2aab21e9ac88>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some sequences are very long which may cause overfitting for small sequences\n",
    "# I will determine a cutoff based on standard deviation\n",
    "RNA_df.len.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = int(RNA_df.len.std()*2)\n",
    "RNA_df['seq_cutted'] = RNA_df['seq'].apply(lambda x: x[0:max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA_df['kmers'] = RNA_df['seq_cutted'].apply(lambda x: seq_to_kmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>len</th>\n",
       "      <th>class</th>\n",
       "      <th>seq_cutted</th>\n",
       "      <th>kmers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000473358.1</th>\n",
       "      <td>ENST00000473358.1</td>\n",
       "      <td>GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...</td>\n",
       "      <td>712</td>\n",
       "      <td>0</td>\n",
       "      <td>GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...</td>\n",
       "      <td>gtgcac tgcaca gcacac cacacg acacgg cacggc acgg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000461467.1</th>\n",
       "      <td>ENST00000461467.1</td>\n",
       "      <td>GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...</td>\n",
       "      <td>590</td>\n",
       "      <td>0</td>\n",
       "      <td>GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...</td>\n",
       "      <td>ggggtt gggttt ggtttc gtttcg tttcgg ttcggg tcgg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000495576.1</th>\n",
       "      <td>ENST00000495576.1</td>\n",
       "      <td>TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...</td>\n",
       "      <td>1319</td>\n",
       "      <td>1</td>\n",
       "      <td>TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...</td>\n",
       "      <td>tcagcc cagcct agcctc gcctcc cctccc ctccca tccc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000471248.1</th>\n",
       "      <td>ENST00000471248.1</td>\n",
       "      <td>GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...</td>\n",
       "      <td>629</td>\n",
       "      <td>0</td>\n",
       "      <td>GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...</td>\n",
       "      <td>gaagct aagctc agctcg gctcga ctcgag tcgagg cgag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000466557.6</th>\n",
       "      <td>ENST00000466557.6</td>\n",
       "      <td>ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...</td>\n",
       "      <td>1301</td>\n",
       "      <td>0</td>\n",
       "      <td>ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...</td>\n",
       "      <td>atgatg tgatga gatgat atgatt tgatta gattat atta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "id                                     \n",
       "ENST00000473358.1  ENST00000473358.1   \n",
       "ENST00000461467.1  ENST00000461467.1   \n",
       "ENST00000495576.1  ENST00000495576.1   \n",
       "ENST00000471248.1  ENST00000471248.1   \n",
       "ENST00000466557.6  ENST00000466557.6   \n",
       "\n",
       "                                                                 seq   len  \\\n",
       "id                                                                           \n",
       "ENST00000473358.1  GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...   712   \n",
       "ENST00000461467.1  GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...   590   \n",
       "ENST00000495576.1  TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...  1319   \n",
       "ENST00000471248.1  GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...   629   \n",
       "ENST00000466557.6  ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...  1301   \n",
       "\n",
       "                   class                                         seq_cutted  \\\n",
       "id                                                                            \n",
       "ENST00000473358.1      0  GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCC...   \n",
       "ENST00000461467.1      0  GGGGTTTCGGGGCTGTGGACCCTGTGCCAGGAAAGGAAGGGCGCAG...   \n",
       "ENST00000495576.1      1  TCAGCCTCCCAAGTAGCTGGGGCTACAGGCACCTGCCACCAAACCC...   \n",
       "ENST00000471248.1      0  GAAGCTCGAGGAAGAGAAAAAAAAACTGGAAGGAGAAATCATAGAT...   \n",
       "ENST00000466557.6      0  ATGATGATTATTCCCCACCTTCTAAGAGACAAAGACCAACGAGCCA...   \n",
       "\n",
       "                                                               kmers  \n",
       "id                                                                    \n",
       "ENST00000473358.1  gtgcac tgcaca gcacac cacacg acacgg cacggc acgg...  \n",
       "ENST00000461467.1  ggggtt gggttt ggtttc gtttcg tttcgg ttcggg tcgg...  \n",
       "ENST00000495576.1  tcagcc cagcct agcctc gcctcc cctccc ctccca tccc...  \n",
       "ENST00000471248.1  gaagct aagctc agctcg gctcga ctcgag tcgagg cgag...  \n",
       "ENST00000466557.6  atgatg tgatga gatgat atgatt tgatta gattat atta...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation data is important for neural network backpropagation\n",
    "X=RNA_df.drop(columns=['class','id','len','seq','seq_cutted'])\n",
    "y=RNA_df.loc[:,'class']\n",
    "train, test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=15)\n",
    "# The test data is the same but I believe I need to  take the validatin data from the train\n",
    "train, val, y_train, y_val = train_test_split(train,y_train,test_size=0.15, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "tokenizer = Tokenizer(split=' ')\n",
    "tokenizer.fit_on_texts(train.values.tolist())\n",
    "X_train = tokenizer.texts_to_sequences(train.values.tolist())\n",
    "X_test = tokenizer.texts_to_sequences(test.values.tolist())\n",
    "X_val = tokenizer.texts_to_sequences(val.values.tolist())\n",
    "# Padding the sequence of vectors with zeros - Is this okay to do? Read about it online and I dont think it will mess anything up?\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "#  Keras Documentation\n",
    "# pad_sequences is used to ensure that all sequences in a list have the same length. \n",
    "# By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the \n",
    "# longest sequence. https://stackoverflow.com/questions/42943291/what-does-keras-io-preprocessing-sequence-pad-sequences-do\n",
    "X_train = pad_sequences(X_train,maxlen = max_len)\n",
    "X_test = pad_sequences(X_test,maxlen = max_len )\n",
    "X_val = pad_sequences(X_val,maxlen = max_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2703, 1606)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = X_train.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the output to categorical classes\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)\n",
    "Y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate class weights\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  np.unique(Y_train.argmax(-1)),\n",
    "                                                  Y_train.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lecture 13: RNN 2 - Classification https://www.youtube.com/watch?v=1vGOQAel2yU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_classifier(bidirectional, add_attention,trained_embedding):\n",
    "    inp1 = Input(shape=(X_train.shape[1],),dtype='uint16')    \n",
    "    if trained_embedding:\n",
    "        emb = embedding\n",
    "        main = emb(inp1)\n",
    "    else:\n",
    "        main = Embedding(max_features,120)(inp1)\n",
    "\n",
    "    if bidirectional ==True and add_attention ==False:\n",
    "        main = Bidirectional(LSTM(16, return_sequences=False))(main)\n",
    "     \n",
    "    elif add_attention ==True:\n",
    "        main = Bidirectional(LSTM(16, return_sequences=True))(main)\n",
    "        main = Attention_layer()(main)     \n",
    "    else:\n",
    "        main = LSTM(16, return_sequences=False)(main)\n",
    "        \n",
    "    main = Activation('relu')(main)\n",
    "    main = BatchNormalization()(main)\n",
    "    main = Dropout(0.5)(main)\n",
    "    main = Dense(128)(main) \n",
    "    main = Dropout(0.5)(main)\n",
    "    main = Dense(64)(main) \n",
    "    main = Dropout(0.5)(main)\n",
    "    main = BatchNormalization()(main)\n",
    "    out = Dense(2,activation='sigmoid')(main)\n",
    "    \n",
    "    model = Model(inputs=[inp1], outputs=[out])\n",
    "    optimizer = adam(0.001,amsgrad=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to implement the Attention Layer in Keras - https://www.youtube.com/watch?v=oaV_Fv5DwUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention Implementation in Keras from https://github.com/BITDM/bitdm.github.io\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class Attention_layer(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (LSTM/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        super(Attention_layer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "\n",
    "        a = K.exp(uit)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) https://www.youtube.com/watch?v=WCUNPb-5EYI\n",
    "# Coursera Sequence Models   Long Short Term Memory (LSTM) - https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay\n",
    "# Deep Learning with Tensorflow - The Long Short Term Memory Model https://www.youtube.com/watch?v=xPotjBiIFFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amandam1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/amandam1/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/amandam1/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2703 samples, validate on 477 samples\n",
      "Epoch 1/50\n",
      " - 130s - loss: 0.8243 - acc: 0.5081 - val_loss: 0.7017 - val_acc: 0.4990\n",
      "Epoch 2/50\n",
      " - 135s - loss: 0.7684 - acc: 0.5261 - val_loss: 0.6985 - val_acc: 0.5010\n",
      "Epoch 3/50\n",
      " - 134s - loss: 0.6442 - acc: 0.6408 - val_loss: 1.0351 - val_acc: 0.4759\n",
      "Epoch 4/50\n",
      " - 113s - loss: 0.3522 - acc: 0.8491 - val_loss: 0.7086 - val_acc: 0.5304\n",
      "Epoch 5/50\n",
      " - 114s - loss: 0.1772 - acc: 0.9521 - val_loss: 1.7037 - val_acc: 0.5262\n",
      "Epoch 6/50\n",
      " - 114s - loss: 0.1339 - acc: 0.9728 - val_loss: 1.0232 - val_acc: 0.4759\n",
      "Epoch 7/50\n",
      " - 114s - loss: 0.1114 - acc: 0.9782 - val_loss: 1.0409 - val_acc: 0.4759\n",
      "795/795 [==============================] - 4s 5ms/step\n",
      "LSTM Model Accuracy : 0.5012578616352201\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=False,add_attention=False,trained_embedding=False)\n",
    "checkpoint = ModelCheckpoint(filepath='LSTM.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "#model.load_weights('2input_BiLSTM.hdfs')\n",
    "\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('LSTM.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional RNN  https://www.coursera.org/lecture/nlp-sequence-models/bidirectional-rnn-fyXnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 165s - loss: 0.8817 - acc: 0.4941 - val_loss: 0.6991 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 196s - loss: 0.8288 - acc: 0.5070 - val_loss: 0.7165 - val_acc: 0.4167\n",
      "Epoch 3/50\n",
      " - 136s - loss: 0.7776 - acc: 0.5298 - val_loss: 0.6819 - val_acc: 0.5833\n",
      "Epoch 4/50\n",
      " - 152s - loss: 0.6709 - acc: 0.6229 - val_loss: 0.6989 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 147s - loss: 0.4615 - acc: 0.7859 - val_loss: 0.6911 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      " - 175s - loss: 0.2955 - acc: 0.8830 - val_loss: 0.7912 - val_acc: 0.4167\n",
      "Epoch 7/50\n",
      " - 175s - loss: 0.2335 - acc: 0.9227 - val_loss: 0.7900 - val_acc: 0.4167\n",
      "Epoch 8/50\n",
      " - 156s - loss: 0.1968 - acc: 0.9393 - val_loss: 0.7258 - val_acc: 0.4167\n",
      "400/400 [==============================] - 9s 23ms/step\n",
      "LSTM Model Accuracy : 0.505\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=True,add_attention=False,trained_embedding=False)\n",
    "checkpoint = ModelCheckpoint(filepath='Bi_LSTM.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('Bi_LSTM.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coursera Sequence Models -  Attention Model Intuition https://www.coursera.org/lecture/nlp-sequence-models/attention-model-intuition-RDXpX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 159s - loss: 0.7223 - acc: 0.4937 - val_loss: 0.6974 - val_acc: 0.5833\n",
      "Epoch 2/50\n",
      " - 169s - loss: 0.7208 - acc: 0.4812 - val_loss: 0.7334 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      " - 176s - loss: 0.7098 - acc: 0.5202 - val_loss: 1.6002 - val_acc: 0.5833\n",
      "Epoch 4/50\n",
      " - 184s - loss: 0.6951 - acc: 0.5298 - val_loss: 6.6793 - val_acc: 0.5833\n",
      "Epoch 5/50\n",
      " - 162s - loss: 0.6512 - acc: 0.6207 - val_loss: 6.6793 - val_acc: 0.5833\n",
      "Epoch 6/50\n",
      " - 151s - loss: 0.4846 - acc: 0.8079 - val_loss: 6.6793 - val_acc: 0.5833\n",
      "400/400 [==============================] - 9s 23ms/step\n",
      "Attention_Bi_LSTM Model Accuracy : 0.505\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM + Attention Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=True,add_attention=True,trained_embedding=False)\n",
    "checkpoint = ModelCheckpoint(filepath='Attention_Bi_LSTM.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('Attention_Bi_LSTM.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('Attention_Bi_LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid the MLP neural network \n",
    "def build_mlp_classifier():\n",
    "    #the input must have the number of features\n",
    "    inp1 = Input(shape=(X_train.shape[1],),dtype='float')\n",
    "    emb =Embedding(max_features,120)(inp1)\n",
    "    main = Dense(64)(emb)\n",
    "    # Dropouts are important t prevent the overfitting\n",
    "    main = Dropout(0.5)(main)\n",
    "    # Batch normalization allow faster convergence\n",
    "    main = BatchNormalization()(main)\n",
    "    main = Dense(64)(main)  \n",
    "    main = Dropout(0.5)(main)\n",
    "    main = BatchNormalization()(main)\n",
    "    main = GlobalMaxPooling1D()(main)\n",
    "    main = Dense(32)(main)  \n",
    "    #main = Dropout(0.5)(main)\n",
    "    #main = Dense(256)(main)  \n",
    "    #main = Dropout(0.5)(main)\n",
    "    # Simoid function is a must in binary classification\n",
    "    out = Dense(2,activation='sigmoid')(main)\n",
    "    model = Model(inputs=[inp1], outputs=[out])\n",
    "    # Slower leraning rate is important in small dataset < 0.001\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 37s - loss: 0.8657 - acc: 0.5118 - val_loss: 0.6866 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      " - 34s - loss: 0.7531 - acc: 0.4930 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      " - 33s - loss: 0.7569 - acc: 0.4941 - val_loss: 0.6919 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      " - 34s - loss: 0.7368 - acc: 0.4982 - val_loss: 0.6899 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      " - 33s - loss: 0.7419 - acc: 0.4923 - val_loss: 0.6894 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      " - 34s - loss: 0.7286 - acc: 0.5132 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 7/50\n",
      " - 33s - loss: 0.7270 - acc: 0.5151 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 8/50\n",
      " - 34s - loss: 0.7232 - acc: 0.5132 - val_loss: 0.6866 - val_acc: 0.5833\n",
      "Epoch 9/50\n",
      " - 35s - loss: 0.7314 - acc: 0.5184 - val_loss: 0.6877 - val_acc: 0.5833\n",
      "Epoch 10/50\n",
      " - 36s - loss: 0.7186 - acc: 0.5258 - val_loss: 0.6890 - val_acc: 0.5833\n",
      "Epoch 11/50\n",
      " - 36s - loss: 0.7115 - acc: 0.5397 - val_loss: 0.6897 - val_acc: 0.5833\n",
      "Epoch 12/50\n",
      " - 35s - loss: 0.7168 - acc: 0.5140 - val_loss: 0.6903 - val_acc: 0.5833\n",
      "400/400 [==============================] - 2s 6ms/step\n",
      "MLP_sequence_based Model Accuracy : 0.505\n"
     ]
    }
   ],
   "source": [
    "# MLP_sequence_based\n",
    "batch_size = 32\n",
    "model = build_mlp_classifier()\n",
    "checkpoint = ModelCheckpoint(filepath='MLP_sequence_based.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('MLP_sequence_based.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('MLP_sequence_based Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Google W2V will take a lot of memory\n",
    "from gensim.models import  KeyedVectors,Word2Vec \n",
    "import multiprocessing\n",
    "\n",
    "# Gensim Word2Vec model\n",
    "def get_embedding(x_array,tokenizer,max_features):\n",
    "    # Function for getting word2ve ang keras embedding layer\n",
    "    df = pd.DataFrame()\n",
    "    df['x'] = tokenizer.sequences_to_texts(x_array)\n",
    "    corpus = df['x'].apply(lambda x: x.split(' '))\n",
    "\n",
    "    # Create Word2Vec\n",
    "    word2vec = Word2Vec(sentences=corpus,\n",
    "                        #size=120,#max_vocab_size=max_features,\n",
    "                        hs=1,\n",
    "                        seed=1,\n",
    "                        workers=multiprocessing.cpu_count())\n",
    "    embedding = word2vec.wv.get_keras_embedding(train_embeddings=False)\n",
    "    return word2vec, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Getting the embedding from the word2vec\n",
    "word2vec,embedding = get_embedding(X_train,tokenizer,max_features)\n",
    "word2vec.save(\"text_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 84s - loss: 0.8494 - acc: 0.5007 - val_loss: 0.7347 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 86s - loss: 0.8169 - acc: 0.4982 - val_loss: 0.6960 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      " - 85s - loss: 0.7901 - acc: 0.5022 - val_loss: 0.6977 - val_acc: 0.4167\n",
      "Epoch 4/50\n",
      " - 73s - loss: 0.7739 - acc: 0.5022 - val_loss: 0.7006 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 83s - loss: 0.7414 - acc: 0.5269 - val_loss: 0.7021 - val_acc: 0.4167\n",
      "Epoch 6/50\n",
      " - 72s - loss: 0.7482 - acc: 0.5166 - val_loss: 0.7002 - val_acc: 0.4167\n",
      "Epoch 7/50\n",
      " - 79s - loss: 0.7509 - acc: 0.5022 - val_loss: 0.6993 - val_acc: 0.4167\n",
      "400/400 [==============================] - 7s 17ms/step\n",
      "LSTM Model Accuracy : 0.5\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=False,add_attention=False,trained_embedding=True)\n",
    "checkpoint = ModelCheckpoint(filepath='LSTM_w2v.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "#model.load_weights('2input_BiLSTM.hdfs')\n",
    "\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('LSTM_w2v.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 118s - loss: 0.8716 - acc: 0.4930 - val_loss: 0.6953 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 113s - loss: 0.8084 - acc: 0.4956 - val_loss: 0.7081 - val_acc: 0.4167\n",
      "Epoch 3/50\n",
      " - 112s - loss: 0.7742 - acc: 0.5081 - val_loss: 0.7092 - val_acc: 0.4167\n",
      "Epoch 4/50\n",
      " - 114s - loss: 0.7780 - acc: 0.5088 - val_loss: 0.7041 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 116s - loss: 0.7674 - acc: 0.5140 - val_loss: 0.7057 - val_acc: 0.4167\n",
      "Epoch 6/50\n",
      " - 125s - loss: 0.7561 - acc: 0.5283 - val_loss: 0.7062 - val_acc: 0.4167\n",
      "400/400 [==============================] - 8s 20ms/step\n",
      "LSTM Model Accuracy : 0.495\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=True,add_attention=False,trained_embedding=True)\n",
    "checkpoint = ModelCheckpoint(filepath='Bi_LSTM_w2v.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('Bi_LSTM_w2v.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('LSTM Model Accuracy : '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1359 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      " - 130s - loss: 0.7367 - acc: 0.4960 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 2/50\n",
      " - 115s - loss: 0.7188 - acc: 0.5092 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 3/50\n",
      " - 99s - loss: 0.7206 - acc: 0.5136 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 4/50\n",
      " - 98s - loss: 0.7124 - acc: 0.5125 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 5/50\n",
      " - 100s - loss: 0.7077 - acc: 0.4956 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "Epoch 6/50\n",
      " - 97s - loss: 0.7010 - acc: 0.5022 - val_loss: 9.3510 - val_acc: 0.4167\n",
      "400/400 [==============================] - 8s 19ms/step\n",
      "Attention_Bi_LSTM Model Accuracy : 0.495\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM + Attention Model\n",
    "batch_size = 32\n",
    "model = build_lstm_classifier(bidirectional=True,add_attention=True,trained_embedding=True)\n",
    "checkpoint = ModelCheckpoint(filepath='Attention_Bi_LSTM_w2v.hdfs', save_weights_only=True,\n",
    "                             monitor='val_loss',save_best_only=True)\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                          verbose=0, mode='auto')\n",
    "RL = ReduceLROnPlateau(min_lr=0.00001,factor=0.3,patience=2)\n",
    "\n",
    "model.fit(X_train,Y_train,callbacks=[checkpoint,RL,EarlyStop],class_weight=class_weights,\n",
    "          validation_data=[X_val, Y_val],\n",
    "          epochs=50,\n",
    "          batch_size=batch_size, \n",
    "          verbose=2)\n",
    "model.load_weights('Attention_Bi_LSTM_w2v.hdfs')\n",
    "acc = model.evaluate(X_test,Y_test)[1]\n",
    "print('Attention_Bi_LSTM Model Accuracy : '+ str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
